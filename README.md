# Pacman
AI Gameplay

An AI to play the Atari game Pacman

Implementation: Multi-Agent Search using Minimax with Alpha-Beta Pruning

Authors: Theodore Klausner, Glenn Baevsky, Akshay Yadava

Application Setting: Open-source Pacman game code found at https://pacmancode.com/

Running the AI: command <python3 run.py> using Pygame, a cross-platform set of Python modules designed for writing video games
  
Changing the Depth of the Minimax Search Tree: In run.py change self.depth in the GameController class

Multi-Agent Search:

For our Multi-Agent Search, we decided to implement the Minimax Algorithm to simulate games a designated number of turns ahead.  As our algorithm improved on time and efficiency, we were able to increase the amount of moves (depths in our game tree) the algorithm considered.  The first step to our algorithm was getting a working game state that could be modified as agents made a succeeding move.  In our game state we put Pacman’s current position, the four ghosts’ positions, a list of legal moves each of these five characters can move in without hitting a barrier or maze edge, a list of the pellets still available to be eaten by Pacman, and a frame counter.  This gave us all the information we needed to eventually be passed into our game state evaluation function once a leaf node was reached.

To get each of these game state attributes we had to implement a lot of helper functions within the Pacman, Ghost, and Pellet classes in order to update a simulated game without tampering with the game state of the live game.  This initially posed a challenge for us as we were struggling to separate simulated games in the Minimax tree from the actual live game state.  Since the source code of the game had no documentation, we struggled with understanding the purpose of each of these functions in the separate classes.  We had to run many iterations of the game to understand which built-in functions dealt with updating the game’s agents and items.  After that, it was a matter of building new functions off of these functions to update our simulated game state.

Just like the actual game does, we treated each of the four ghosts and Pacman as a separate agent alternating whose turn it is to move to their next position.  Each time one of these agents moved, the game state had to be updated before proceeding to the next depth in the tree.  In order to handle this we created functions that returned the new position of these agents without updating their actual position in the game, and functions that returned all possible legal moves that could be made by that agent in their next iteration.  Updating the list of pellets available was not so difficult.  We simply checked whether or not Pacman collided with a pellet each time they moved and if so removed that pellet from the game state.  
  
Our biggest challenge in predicting future game states was tracking the individual ghost’s movement.  Each ghost in the game has its own personality, some head towards Pacman and others randomly pick a goal and move towards that goal.  Since these opponents do not act optimally to minimize the player’s score, our Minimax algorithm would not have been perfect unless we accounted for this.  Also, as levels advance, the opponents become smarter and faster as more ghosts flood to hit Pacman quicker.  This made it so we could eliminate nodes on our game tree that dealt with ghosts moving in positions they would not necessarily move in…unless they entered Freight Mode.  If at some point in our game tree path that Pacman had hit a power pellet, ghosts would start fleeing in unpredicted ways, making it very difficult to simulate those movements.  

The source code defined this way of movement as Scatter, where the ghosts pick a random direction to move away from Pacman to avoid being eaten.  This logic posed a huge struggle for us we were not able to overcome in our game simulations. As we started to get to higher depths in our Minimax tree, our runtime increased significantly.  To combat this issue we implemented a key feature to our AI: Alpha-Beta Pruning.  Alpha-Beta Pruning further decreased our runtime as we got lower depths, pruning large sections of the tree that were unnecessary to iterate through.


Evaluation Functions:

After Minimiax was implemented for our Multi-Agent search, we then were tasked with creating an evaluation function. This function scores a game state at a given time in order to return the most optimal move for an agent at that position. The main goal of the game is to to stay alive as long as possible by evading enemy ghosts while maximizing points. When Pacman is traversing the search space, there are many factors it must assess in order to make the most optimal move. Initially, we decided to implement the evaluation function by assessing the total distance to each pellet and each ghost in a given game state. For the ghosts, we decided to sum the total Manhattan distance of a ghost to Pacman’s current position.  Greater distances to the ghosts are seen as better and thus receive a higher score for that game state. For tracking the location of food pellets, we similarly implemented this with the Manhattan distance, though shorter locations to food pellets were seen as better and further distances were worse. This led us to add the reciprocal of the distance to food pellets in a new game state to our evaluation function score. Although we had some issues with weighting the various components of our evaluation function, these two factors created our first initial implementation of our function. 

Upon running a few initial tests of our algorithm, the AI was actively trying to eat pellets while maximizing its distance to ghosts.  This caused an issue to arise. Even when there were surplus pellets in a local region and some located in a different quadrant of the board, the AI would still try to move in the direction of pellets further away, while undermining the pellets located locally to the agent. To combat this issue, we added another factor to our evaluation function, the amount of pellets located in a local region. This worked by summing up the scores of pellets in a radius of 50 units to Pacman in this static game state.  We then continuously decrease this radius by 10 and recalculate how many pellets remain locally. The intuition was that this factor of the evaluation function would now weigh local pellets more than the total pellets located on the board, prioritizing those closer to the agent. Upon implementing this into our algorithm, we found that the Pacman AI would now target local pellets even if there were some present in a different location of the board, solving our initial problem. Still, a new problem arose. Even though we assessed the total distances to ghosts in this game state, when a ghost came near a Pacman, it would not always move in the optimal direction to maximize its distance to the ghost because of the presence of these other factors. 

This led us to implement one more factor into our algorithm, an insurance to check that the ghost will choose not to move to a game state that is within a certain distance to a ghost. This part of the algorithm returned an extremely low score when processing the Manhattan distances to the ghost. In some instances, all the possible moves a Pacman could make in a given game state would be within the given radius to a ghost, thus not having a proper way of evading them in that situation. One final assurance check added to the algorithm was that whenever a ghost was within 5 space locations to the Pacman, the only criteria of the evaluation function would be to maximize Pacman’s distance to the nearest ghost. This final factor completed our implementation of our evaluation function.

